{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries and declaring some global variables\n",
    "import pandas as  pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "bagOfWords = []\n",
    "unique_words = set() # the entire vocabluary\n",
    "uw0 = set() # class 0 vocabluary\n",
    "uw1 = set() # class 1 vocabluary\n",
    "freq0 ={} # dictionary to count the occurence of words in class 0 reviews\n",
    "freq1 ={} # dictionary to count the occurence of words in class 0 reviews\n",
    "p_c0,p_c1 =0,0 # prior probability\n",
    "n0,n1 = 0 ,0  # number of class 0 and class 1 reviews\n",
    "d0,d1,d2 =  pd.DataFrame(),pd.DataFrame(),pd.DataFrame()\n",
    "d3,d4,D =pd.DataFrame(),pd.DataFrame(),pd.DataFrame()# folds and mterics\n",
    "acc,fscores,datafolds = [],[],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train):\n",
    "    global bagOfWords,unique_words,p_c0,p_c1,n0,n1,freq0,freq1,uw0,uw1\n",
    "    uw0 = set()\n",
    "    uw1 = set()\n",
    "    p_c0,p_c1 =0,0\n",
    "    n0,n1 = 0 ,0\n",
    "\n",
    "    bagOfWords = [] # each preprocessed training review as an array of words \n",
    "    for ind ,row in train.iterrows():\n",
    "        bag = [w for w in row['Review'].split() if w!=' '] \n",
    "        bagOfWords.append(bag)\n",
    "        for w in bag:\n",
    "            if row['Label']=='0':\n",
    "                uw0.add(w) # building the vocabulary for class 0\n",
    "            else:\n",
    "                uw1.add(w) # building the vocabulary for class 1\n",
    "       \n",
    "    n0 = len(train.loc[train['Label']=='0'])\n",
    "    n1 = len(train.loc[train['Label']=='1'])\n",
    "   \n",
    "    # print(n0,n1) # no of class 0  and class 1 review\n",
    "    unique_words = set()\n",
    "    unique_words = uw0.union(uw1)\n",
    "   # print(len(unique_words)) # voacbulary size\n",
    "    freqvect = [] # arr of dicts that store the occurence of each word in a review\n",
    "    for i in range(len(bagOfWords)):\n",
    "        d = {}\n",
    "        #d = dict.fromkeys(unique_words,0)\n",
    "        for word in bagOfWords[i]:\n",
    "            if word in d:\n",
    "                d[word]+=1 #d[word]+=1\n",
    "            else:\n",
    "                d[word]=1\n",
    "        freqvect.append(d)\n",
    "    \n",
    "    freq0 = {} # occurence of each word that are present in reviews for which class==0\n",
    "    freq1 = {}  # occurence of each word that are present in reviews for which class==1\n",
    "    p_c0,p_c1 =0,0\n",
    "    for word in unique_words:\n",
    "        c0,c1 = 0,0\n",
    "        for i in range(train.shape[0]):\n",
    "            if word in freqvect[i]:\n",
    "                if train.iloc[i,1]=='0':\n",
    "                    c0+=freqvect[i][word]\n",
    "                    #p_c0+=1\n",
    "                else:\n",
    "                    c1+=freqvect[i][word]\n",
    "                #p_c1+=1\n",
    "        freq0[word]=c0+1 # adding 1 to occurence of each word to implement laplacian smoothening\n",
    "        freq1[word]=c1+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test):\n",
    "    global freq0,freq1,unique_words,n0,n1,uw0,uw1\n",
    "    \n",
    "    bowtest = [] # each preprocessed testing review as an array of words\n",
    "    for ind ,row in test.iterrows():\n",
    "        bag = [w for w in row['Review'].split() if w!=' '] \n",
    "        bowtest.append(bag)\n",
    "    \n",
    "    probtest0 = [] # to store prob that ith review belong to class 0\n",
    "    probtest1 = [] # to store prob that ith review belong to class 1\n",
    "    p_c0 = n0/(n0+n1) # prior prob of class 0\n",
    "    p_c1 = n1/(n0+n1) # prior prob of class 1\n",
    "    for i in range(len(bowtest)):\n",
    "        p0,p1=1,1\n",
    "        for word in bowtest[i]:\n",
    "            if word in unique_words:\n",
    "                p0*=(freq0[word]/(len(uw0)+len(unique_words))) # +2 adding 10 to the denom because of laplacian smoothening\n",
    "                p1*=(freq1[word]/(len(uw1)+len(unique_words)))\n",
    "            else:\n",
    "                p0*=(1/(len(uw0)+len(unique_words)))\n",
    "                p1*=(1/(len(uw1)+len(unique_words)))\n",
    "        probtest0.append(p0*p_c0)\n",
    "        probtest1.append(p1*p_c1)\n",
    "    \n",
    "    pred = [] # storing the predicted labels\n",
    "    for i,j in zip(probtest0,probtest1):\n",
    "        if(i>j):\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    \n",
    "    actu = np.asarray([int(s) for s in test.iloc[:,1]])\n",
    "    pred = np.asarray(pred)\n",
    "    y_actu = pd.Series(actu, name='Actual')\n",
    "    y_pred = pd.Series(pred, name='Predicted')\n",
    "    # making the comfusion matrix\n",
    "    cm = pd.crosstab(y_actu, y_pred)\n",
    "    \n",
    "    acc = (cm.iloc[0,0]+cm.iloc[1,1])/len(test) # accuracy\n",
    "    p = cm.iloc[0,0]/(cm.iloc[0,0]+cm.iloc[1,0]) # precision\n",
    "    r = cm.iloc[0,0]/(cm.iloc[0,0]+cm.iloc[0,1]) # recall\n",
    "    f_score = 2*p*r/(p+r)\n",
    "    return (acc,round(f_score,4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_data(lines):\n",
    "    linesasarr=[] # to store the review and label as an array\n",
    "    for line in lines:\n",
    "        arr = [str(word) for word in line.split()] # splitting each word\n",
    "        line=''\n",
    "        for i in range(len(arr)-1): # the last element is the label so leaving it out from the review\n",
    "            line+=arr[i]+\" \"\n",
    "        line+=','+arr[len(arr)-1] # appending the label to the review separated by a comma\n",
    "        linesasarr.append(line)\n",
    "    lines = []\n",
    "    # separating the review and the label and appending it to a dataframe\n",
    "    for line in linesasarr:\n",
    "        ind = line.rindex(',')\n",
    "        review,label = line[0:ind],line[ind+1:]\n",
    "        arr= [review,label]\n",
    "        lines.append(arr)\n",
    "    data= pd.DataFrame(lines,columns=['Review','Label'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(unclean):\n",
    "    for ind ,row in unclean.iterrows():\n",
    "        review = row['Review'].lower() # to lower case\n",
    "        filtered_review = ''\n",
    "        for word in review.split():\n",
    "            if not word in stopwords: # removing stopwords\n",
    "                filtered_review+=word+\" \"\n",
    "        review = filtered_review\n",
    "        filtered_review=''\n",
    "        for word in review.split():\n",
    "            alpha_only =  re.sub('[^a-zA-Z]','' ,word) # removing symbols\n",
    "            if alpha_only!=' ' and not alpha_only in stopwords: # double checking for stopwords\n",
    "                filtered_review+= alpha_only+\" \"\n",
    "        row['Review'] = filtered_review.strip() # removing unwanted spaces\n",
    "    return unclean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data):\n",
    "    global d0,d1,d2,d3,d4,D,datafolds\n",
    "    d0,d1,d2,d3,d4,D = pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame(),pd.DataFrame()\n",
    "    # using pandas to create 5 folds of the entire data set\n",
    "    d0 = data.sample(frac=0.2,random_state=29)\n",
    "    D = data.drop(d0.index)\n",
    "    d1 = D.sample(frac=0.25,random_state=19)\n",
    "    D = D.drop(d1.index)\n",
    "    d2 = D.sample(frac=1/3,random_state=21)\n",
    "    D = D.drop(d2.index)\n",
    "    d3 = D.sample(frac=0.5,random_state=260)\n",
    "    D = D.drop(d3.index)\n",
    "    d4 = D\n",
    "    datafolds= [d0,d1,d2,d3,d4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run5folds():\n",
    "    global d0,d1,d2,d3,d4 , acc,fscores,datafolds\n",
    "    acc =[] # array to store accuracy and fscore of each iteration\n",
    "    fscores = []\n",
    "    for i in range(len(datafolds)):\n",
    "        train = pd.DataFrame()\n",
    "        test = pd.DataFrame()\n",
    "        for j in range(len(datafolds)):\n",
    "            if j!=i:\n",
    "                train =  train.append(datafolds[j]) # building the training data using 4 folds and leaving 1 out\n",
    "            else:\n",
    "                test = datafolds[j] # the leftout set for testing\n",
    "        #print(len(train),len(test))\n",
    "        build_model(train)\n",
    "        a,f = predict(test)\n",
    "        print('acc = ',a,' fscore = ',f)\n",
    "\n",
    "        #print(a,f)\n",
    "        acc.append(a)\n",
    "        fscores.append(f)\n",
    "        \n",
    "    mean = np.mean(acc)\n",
    "    std = np.std(acc)\n",
    "    print(\"\\nAccuracy \" , round(mean,4),\" +- \",round(std,4))\n",
    "    mean = np.mean(fscores)\n",
    "    std = np.std(fscores)\n",
    "    print(\"FScore \",round(mean,4),\" +- \",round(std,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not preprocessed \n",
      "\n",
      "acc =  0.755  fscore =  0.7435\n",
      "acc =  0.745  fscore =  0.7151\n",
      "acc =  0.715  fscore =  0.6885\n",
      "acc =  0.805  fscore =  0.8\n",
      "acc =  0.715  fscore =  0.6667\n",
      "\n",
      "Accuracy  0.747  +-  0.0331\n",
      "FScore  0.7228  +-  0.0464\n",
      "\n",
      "Data preprocessed \n",
      "\n",
      "acc =  0.815  fscore =  0.7886\n",
      "acc =  0.755  fscore =  0.6879\n",
      "acc =  0.745  fscore =  0.7182\n",
      "acc =  0.775  fscore =  0.7486\n",
      "acc =  0.75  fscore =  0.7059\n",
      "\n",
      "Accuracy  0.768  +-  0.0256\n",
      "FScore  0.7298  +-  0.0354\n"
     ]
    }
   ],
   "source": [
    "f = open('a1_d3.txt','r') # reading the dataset i.e a txt file\n",
    "lines = f.readlines() # each line is a review along with its label \n",
    "f.close()\n",
    "rawdata = reshape_data(lines) \n",
    "f = open('stopwords.txt') # created a textfile containing 191 stopwords\n",
    "lines = f.readlines()\n",
    "stopwords = set()\n",
    "f.close()\n",
    "for line in lines:\n",
    "    stopwords.add(str(line)[:-1])\n",
    "\n",
    "data = rawdata\n",
    "print('Data not preprocessed \\n')\n",
    "create_folds(data)\n",
    "run5folds()\n",
    "\n",
    "print('\\nData preprocessed \\n')\n",
    "data = preprocess(rawdata)\n",
    "create_folds(data)\n",
    "run5folds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
